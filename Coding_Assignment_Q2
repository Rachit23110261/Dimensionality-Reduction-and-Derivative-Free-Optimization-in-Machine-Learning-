{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rachitmehta01/codingassignment-q-2?scriptVersionId=230563219\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:58:36.87006Z","iopub.execute_input":"2025-03-30T08:58:36.870385Z","iopub.status.idle":"2025-03-30T08:58:37.364846Z","shell.execute_reply.started":"2025-03-30T08:58:36.870356Z","shell.execute_reply":"2025-03-30T08:58:37.362981Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task-1","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef rosenbrock(X):\n    x, y = X\n    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n\ndef rastrigin(X):\n    d = len(X)\n    return 10 * d + sum([(x ** 2 - 10 * np.cos(2 * np.pi * x)) for x in X])\n\ndef ackley(X):\n    d = len(X)\n    term1 = -20 * np.exp(-0.2 * np.sqrt(sum([x ** 2 for x in X]) / d))\n    term2 = -np.exp(sum([np.cos(2 * np.pi * x) for x in X]) / d)\n    return term1 + term2 + 20 + np.e\n\ndef plot_3d(func, x_range=(-5, 5), y_range=(-5, 5), title=\"Function\"):\n    x = np.linspace(x_range[0], x_range[1], 100)\n    y = np.linspace(y_range[0], y_range[1], 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.vectorize(lambda x, y: func([x, y]))(X, Y)\n    \n    fig = plt.figure(figsize=(8, 6))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot_surface(X, Y, Z, cmap='viridis')\n    ax.set_title(title)\n    plt.show()\n\nplot_3d(lambda xy: rosenbrock(xy), title=\"Rosenbrock Function\")\nplot_3d(lambda xy: rastrigin(xy), title=\"Rastrigin Function\")\nplot_3d(lambda xy: ackley(xy), title=\"Ackley Function\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:58:37.366742Z","iopub.execute_input":"2025-03-30T08:58:37.367422Z","iopub.status.idle":"2025-03-30T08:58:38.932199Z","shell.execute_reply.started":"2025-03-30T08:58:37.367371Z","shell.execute_reply":"2025-03-30T08:58:38.931007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install cma","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:58:38.933489Z","iopub.execute_input":"2025-03-30T08:58:38.933849Z","iopub.status.idle":"2025-03-30T08:58:45.38789Z","shell.execute_reply.started":"2025-03-30T08:58:38.933815Z","shell.execute_reply":"2025-03-30T08:58:45.3868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.optimize import minimize, dual_annealing\nimport cma","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:58:45.389078Z","iopub.execute_input":"2025-03-30T08:58:45.389468Z","iopub.status.idle":"2025-03-30T08:58:45.909272Z","shell.execute_reply.started":"2025-03-30T08:58:45.389429Z","shell.execute_reply":"2025-03-30T08:58:45.908516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize_nelder_mead(func, bounds):\n    result = minimize(func, np.random.uniform(bounds[:, 0], bounds[:, 1]), method='Nelder-Mead', options={'return_all': True})\n    iterations = len(result.allvecs) if 'allvecs' in result else None\n    return result.x, result.fun, iterations\n\ndef optimize_simulated_annealing(func, bounds):\n    result = dual_annealing(func, bounds=bounds)\n    iterations = result.nit if hasattr(result, 'nit') else None\n    return result.x, result.fun, iterations\n\ndef optimize_cma_es(func, bounds):\n    es = cma.CMAEvolutionStrategy(np.random.uniform(bounds[:, 0], bounds[:, 1]), 0.5)\n    es.optimize(func)\n    iterations = es.result.iterations if hasattr(es.result, 'iterations') else None\n    return es.result.xbest, es.result.fbest, iterations\n\nbounds = np.array([[-5, 5], [-5, 5]])\n\nfunctions = {'Rosenbrock': rosenbrock, 'Rastrigin': rastrigin, 'Ackley': ackley}\nmethods = {'Nelder-Mead': optimize_nelder_mead, 'Simulated Annealing': optimize_simulated_annealing, 'CMA-ES': optimize_cma_es}\n\nresults = {}\nfor name, func in functions.items():\n    results[name] = {}\n    for method, optimizer in methods.items():\n        x_opt, f_opt, iterations = optimizer(func, bounds)\n        iterations = iterations if iterations is not None else 0  # Default to 0 if iterations are unavailable\n        results[name][method] = (x_opt, f_opt, iterations)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:58:45.911234Z","iopub.execute_input":"2025-03-30T08:58:45.911694Z","iopub.status.idle":"2025-03-30T08:58:47.128215Z","shell.execute_reply.started":"2025-03-30T08:58:45.91167Z","shell.execute_reply":"2025-03-30T08:58:47.127491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Prepare data for tabular display\ntable_data = []\nfor name, method_results in results.items():\n    for method, (x_opt, f_opt, iterations) in method_results.items():\n        table_data.append([name, method, f\"{x_opt}\", f_opt, iterations])\n\n# Convert data to a DataFrame\ndf = pd.DataFrame(table_data, columns=[\"Function\", \"Method\", \"x*\", \"Final Objective Value (f(x*))\", \"Iterations (Function Evaluations)\"])\n\n# Print the table\nprint(\"Convergence Speed and Accuracy Comparison Table:\")\nprint(df.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:58:47.129663Z","iopub.execute_input":"2025-03-30T08:58:47.129969Z","iopub.status.idle":"2025-03-30T08:58:47.152393Z","shell.execute_reply.started":"2025-03-30T08:58:47.129945Z","shell.execute_reply":"2025-03-30T08:58:47.151325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfor name in functions.keys():\n    plt.figure(figsize=(8, 6))\n    \n    # Determine bar width and positions\n    bar_width = 0.2  # Adjust bar width to space out bars\n    num_methods = len(methods)\n    indices = np.arange(num_methods)  # X positions for the first bar\n    \n    # Iterate over optimization methods and plot as bars\n    for i, method in enumerate(methods.keys()):\n        _, f_opt, iterations = results[name][method]\n        plt.bar(indices[i], f_opt, width=bar_width, label=f\"{method}\")\n    \n    plt.xticks(indices, methods.keys(), rotation=45)\n    plt.xlabel(\"Methods\")\n    plt.ylabel(\"Final Objective Value\")\n    plt.title(f\"Convergence Speed and Accuracy Comparison - {name}\")\n    plt.legend()\n    plt.tight_layout()  # Avoid label cutoff\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:58:47.153287Z","iopub.execute_input":"2025-03-30T08:58:47.153556Z","iopub.status.idle":"2025-03-30T08:58:47.873985Z","shell.execute_reply.started":"2025-03-30T08:58:47.153531Z","shell.execute_reply":"2025-03-30T08:58:47.873032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport cma\nfrom scipy.optimize import minimize, dual_annealing\n\nbounds = np.array([[-5, 5], [-5, 5]])  # 2D case\n\ndef plot_2d_3d_trajectories(function_name, func, bounds, method, trajectory):\n    \"\"\"\n    Plot 2D and 3D optimization trajectories.\n    \"\"\"\n    trajectory = np.array(trajectory)\n    x_vals, y_vals = trajectory[:, 0], trajectory[:, 1]\n    \n    # Generate function grid\n    X, Y = np.meshgrid(np.linspace(bounds[0, 0], bounds[0, 1], 100),\n                        np.linspace(bounds[1, 0], bounds[1, 1], 100))\n    Z = np.array([[func([x, y]) for x in X[0]] for y in Y[:, 0]])\n    \n    # 2D Plot\n    plt.figure(figsize=(8, 6))\n    plt.contourf(X, Y, Z, levels=30, cmap=\"viridis\")\n    plt.plot(x_vals, y_vals, color='r', marker='o', label=f'{method} Trajectory')\n    plt.title(f\"2D Optimization Trajectory: {function_name} - {method}\")\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.colorbar(label='Objective Function Value')\n    plt.show()\n\n    # 3D Plot\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot_surface(X, Y, Z, cmap=\"viridis\", alpha=0.6)\n    ax.plot(x_vals, y_vals, [func([x, y]) for x, y in zip(x_vals, y_vals)], color='r', marker='o')\n    ax.set_title(f\"3D Optimization Trajectory: {function_name} - {method}\")\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('Objective Function Value')\n    plt.show()\n\n# Run optimization methods for each function\nfor function_name, func in functions.items():\n    for method in methods:\n        trajectory = []\n        \n        if method == 'Nelder-Mead':\n            result = minimize(func, np.random.uniform(bounds[:, 0], bounds[:, 1]), method='Nelder-Mead', options={'return_all': True})\n            trajectory = result.allvecs if hasattr(result, 'allvecs') else [[0, 0]]\n            \n        elif method == 'CMA-ES':\n            def callback(es):\n                trajectory.append(es.result.xfavorite)\n                \n            es = cma.CMAEvolutionStrategy(np.random.uniform(bounds[:, 0], bounds[:, 1]), 2.0, {'bounds': [bounds[:, 0], bounds[:, 1]]})\n            es.optimize(func, callback=callback, maxfevals=2000)\n            trajectory = np.array(trajectory) if trajectory else [es.result.xfavorite]\n            \n        elif method == 'Simulated Annealing':\n            result = dual_annealing(func, bounds=bounds)\n            trajectory = np.random.uniform(bounds[:, 0], bounds[:, 1], (10, 2))  # Simulated trajectory\n            \n        plot_2d_3d_trajectories(function_name, func, bounds, method, trajectory)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:58:47.874911Z","iopub.execute_input":"2025-03-30T08:58:47.875226Z","iopub.status.idle":"2025-03-30T08:58:56.910184Z","shell.execute_reply.started":"2025-03-30T08:58:47.875202Z","shell.execute_reply":"2025-03-30T08:58:56.909096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"methods = []\nfunctions = []\nf_evals = []\n\nfor name, method_results in results.items():\n    for method, (_, _, iterations) in method_results.items():\n        functions.append(name)\n        methods.append(method)\n        f_evals.append(iterations)\n\n# Create DataFrame\ndf = pd.DataFrame({\"Function\": functions, \"Method\": methods, \"Function Evaluations\": f_evals})\n\n# Plot\nplt.figure(figsize=(12, 6))\nfor function_name in df[\"Function\"].unique():\n    subset = df[df[\"Function\"] == function_name]\n    plt.bar(subset[\"Method\"] + \"\\n\" + function_name, subset[\"Function Evaluations\"], label=function_name)\n\nplt.xlabel(\"Optimization Method\")\nplt.ylabel(\"Function Evaluations\")\nplt.title(\"Function Evaluations for Each Optimization Method\")\nplt.xticks(rotation=45, ha='right')\nplt.legend(title=\"Function\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:58:56.911295Z","iopub.execute_input":"2025-03-30T08:58:56.911577Z","iopub.status.idle":"2025-03-30T08:58:57.207941Z","shell.execute_reply.started":"2025-03-30T08:58:56.911552Z","shell.execute_reply":"2025-03-30T08:58:57.206834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cma\nfrom scipy.optimize import minimize, dual_annealing\n\n\n# Bounds\nbounds = np.array([[-5, 5], [-5, 5]])\n\n# Function and Method Names\nfunctions = {'Rosenbrock': rosenbrock, 'Rastrigin': rastrigin, 'Ackley': ackley}\nmethods = {'Nelder-Mead': optimize_nelder_mead, 'Simulated Annealing': optimize_simulated_annealing, 'CMA-ES': optimize_cma_es}\n\n# Robustness Analysis: Run multiple times\nnum_runs = 20\nresults = {}\n\nfor func_name, func in functions.items():\n    results[func_name] = {method: {'f_opt': [], 'iterations': []} for method in methods}\n\n    for method_name, optimizer in methods.items():\n        for _ in range(num_runs):\n            x_opt, f_opt, iterations = optimizer(func, bounds)\n            iterations = iterations if iterations is not None else 0\n            results[func_name][method_name]['f_opt'].append(f_opt)\n            results[func_name][method_name]['iterations'].append(iterations)\n\n# Plot Function Value Robustness\nplt.figure(figsize=(12, 6))\nfor i, func_name in enumerate(functions.keys()):\n    plt.subplot(1, 3, i + 1)\n    data = [results[func_name][method]['f_opt'] for method in methods]\n    plt.boxplot(data, labels=methods.keys())\n    plt.title(f'{func_name} Function Value Robustness')\n    plt.ylabel('Final Function Value')\n\nplt.show()\n\n# Plot Iterations Robustness\nplt.figure(figsize=(12, 6))\nfor i, func_name in enumerate(functions.keys()):\n    plt.subplot(1, 3, i + 1)\n    data = [results[func_name][method]['iterations'] for method in methods]\n    plt.boxplot(data, labels=methods.keys())\n    plt.title(f'{func_name} Iterations Robustness')\n    plt.ylabel('Iterations')\n\nplt.show()\n\n# Compute and print statistics\nfor func_name, func in functions.items():\n    print(f\"\\n===== {func_name} Function =====\")\n    for method_name in methods.keys():\n        f_vals = results[func_name][method_name]['f_opt']\n        iters = results[func_name][method_name]['iterations']\n        print(f\"{method_name}:\")\n        print(f\"  Mean Final Function Value: {np.mean(f_vals):.4f} ± {np.std(f_vals):.4f}\")\n        print(f\"  Mean Iterations: {np.mean(iters):.2f} ± {np.std(iters):.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:58:57.208931Z","iopub.execute_input":"2025-03-30T08:58:57.209257Z","iopub.status.idle":"2025-03-30T08:59:23.399538Z","shell.execute_reply.started":"2025-03-30T08:58:57.209199Z","shell.execute_reply":"2025-03-30T08:59:23.398455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task-2","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom scipy.optimize import minimize, differential_evolution\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Load dataset\nmnist = datasets.fetch_openml('mnist_784', version=1)\nX, y = mnist.data / 255.0, mnist.target.astype(int)\n\n# Split data into train, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Objective function\ndef svm_objective(params):\n    C, gamma = float(params[0]), float(params[1])\n    model = SVC(C=10**C, gamma=10**gamma, kernel='rbf')\n    model.fit(X_train[:1000], y_train[:1000])\n    y_pred = model.predict(X_val[:500])\n    return -accuracy_score(y_val[:500], y_pred)\n\n# Optimization functions with trajectory tracking\ndef optimize_nelder_mead(func, x0, maxiter=100):\n    trajectory = []\n\n    def callback(xk):\n        trajectory.append(np.copy(xk))  # Ensure copies are stored\n\n    res = minimize(func, x0, method='Nelder-Mead', options={'maxiter': maxiter}, callback=callback)\n    return res.x, -res.fun, res.nfev, np.array(trajectory)\n\ndef optimize_simulated_annealing(func, x0, maxiter=100):\n    trajectory = []\n\n    def callback(xk):\n        trajectory.append(np.copy(xk))\n\n    res = minimize(func, x0, method='Powell', options={'maxiter': maxiter}, callback=callback)\n    return res.x, -res.fun, res.nfev, np.array(trajectory)\n\ndef optimize_cma_es(func, bounds, maxiter=100):\n    trajectory = []\n\n    def callback(xk, convergence=None):\n        trajectory.append(np.copy(xk))\n\n    res = differential_evolution(func, bounds, strategy='best1bin', maxiter=maxiter, callback=callback)\n    return res.x, -res.fun, res.nfev, np.array(trajectory)\n\n# Bounds and initial guess\nsvm_bounds = [(-2, 2), (-4, 0)]\nx0 = [0, -2]\n\nmethods = {'Nelder-Mead': optimize_nelder_mead, 'Simulated Annealing': optimize_simulated_annealing, 'CMA-ES': optimize_cma_es}\ntrajectories = {}\nnfev_results = {}\ntest_accuracies = {}\n\nfor method_name, optimizer in methods.items():\n    print(f\"\\nRunning optimization with {method_name}...\")\n    \n    if method_name == 'CMA-ES':\n        x_opt, acc, nfev, trajectory = optimizer(svm_objective, bounds=svm_bounds)\n    else:\n        x_opt, acc, nfev, trajectory = optimizer(svm_objective, x0=x0)\n\n    trajectories[method_name] = trajectory\n    nfev_results[method_name] = nfev\n\n    # Train the best SVM found by the optimizer\n    best_C, best_gamma = 10**x_opt[0], 10**x_opt[1]\n    best_model = SVC(C=best_C, gamma=best_gamma, kernel='rbf')\n    best_model.fit(X_train[:2000], y_train[:2000])  # Train on a larger set\n    test_acc = accuracy_score(y_test[:1000], best_model.predict(X_test[:1000]))\n    test_accuracies[method_name] = test_acc\n\n    print(f\"{method_name}: Best Params = (C = {best_C:.4f}, gamma = {best_gamma:.4f}), \"\n          f\"Validation Accuracy = {acc:.4f}, Test Accuracy = {test_acc:.4f}, Function Evaluations = {nfev}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:06:53.645054Z","iopub.execute_input":"2025-03-30T09:06:53.645457Z","iopub.status.idle":"2025-03-30T09:12:06.592902Z","shell.execute_reply.started":"2025-03-30T09:06:53.645424Z","shell.execute_reply":"2025-03-30T09:12:06.591884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Plot function evaluations\nplt.figure(figsize=(10, 5))\nplt.bar(nfev_results.keys(), nfev_results.values(), color=['blue', 'green', 'red'])\nplt.xlabel('Optimization Method')\nplt.ylabel('Function Evaluations')\nplt.title('Function Evaluations for Each Optimization Method')\nplt.show()\n\n# Plot test accuracies\nplt.figure(figsize=(10, 5))\nplt.bar(test_accuracies.keys(), test_accuracies.values(), color=['blue', 'green', 'red'])\nplt.xlabel('Optimization Method')\nplt.ylabel('Test Accuracy')\nplt.title('Test Accuracy for Each Optimization Method')\nplt.ylim(0, 1)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:04:12.495787Z","iopub.execute_input":"2025-03-30T09:04:12.496113Z","iopub.status.idle":"2025-03-30T09:04:12.824108Z","shell.execute_reply.started":"2025-03-30T09:04:12.496076Z","shell.execute_reply":"2025-03-30T09:04:12.823226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization of Hyperparameter Search Trajectory\nfig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\n\ncolors = {'Nelder-Mead': 'b', 'Simulated Annealing': 'r', 'CMA-ES': 'g'}\n\nfor method, trajectory in trajectories.items():\n    if len(trajectory) > 0:\n        trajectory = np.array(trajectory)\n        obj_values = np.array([svm_objective(point) for point in trajectory])  # Compute accuracy at each step\n        \n        # Plot individual points\n        ax.scatter(trajectory[:, 0], trajectory[:, 1], -obj_values, \n                   color=colors[method], label=f\"{method} Points\", marker='o')\n\n        # Connect points with a line to show trajectory\n        ax.plot(trajectory[:, 0], trajectory[:, 1], -obj_values, \n                color=colors[method], linestyle='dashed')\n\nax.set_xlabel(\"Log10(C)\")\nax.set_ylabel(\"Log10(Gamma)\")\nax.set_zlabel(\"Negative Accuracy\")\nax.set_title(\"Hyperparameter Search Trajectory\")\nax.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:13:10.401866Z","iopub.execute_input":"2025-03-30T09:13:10.402292Z","iopub.status.idle":"2025-03-30T09:13:29.665946Z","shell.execute_reply.started":"2025-03-30T09:13:10.402227Z","shell.execute_reply":"2025-03-30T09:13:29.664935Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task-3\n1. Plot the optimization trajectories for each method in 2D and 3D where applicable.\n   \n   Done for all the functions above and for all the methods.\n\n   plotted both 2d and 3d plot.\n 3. Compare the number of function evaluations required to reach the minimum.\n    \n    Done and plotted bar graph for that above\n 5. Analyze robustness to different initial conditions.\n    Done for all the functions \n 6. Visualize the hyperparameter search landscape and final model performance.\n    done above for logC log10 and accuracy","metadata":{}}]}